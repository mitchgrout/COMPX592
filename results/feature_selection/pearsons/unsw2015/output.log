#!/usr/bin/env python3

import numpy
import ne
import os
import tee
import warnings
from sklearn.naive_bayes import GaussianNB
warnings.filterwarnings('ignore')

if __name__ ==  '__main__':
    DATASET  = ne.data.unsw2015.dataset
    SELECTOR = ne.data.Pearsons
    THRESH   = lambda x: x > 0.5
    
    log_dir = os.path.join('results', 'feature_selection', SELECTOR.name, DATASET.name())
    os.makedirs(log_dir, exist_ok=True)
    with tee.StdoutTee(os.path.join(log_dir, 'output.log'), buff=1):
        ne.util.dump(__file__)

        results = {}
        for n_features in range(1, DATASET.num_features()):
            split = DATASET.data(selector=SELECTOR(n_features), save=False, cache=True)
            model = GaussianNB(var_smoothing=1e-15)
            model.fit(*split.train)
            stats = ne.stats.compute_statistics(THRESH, split.test.ys, model.predict(split.test.xs))
            results[n_features] = stats
            print("{} - {}".format(n_features, stats.mcc))
        print(list(map(lambda t:t[0], sorted(results.items(), key=lambda t:t[1].mcc, reverse=True))))


1 - 0.8415757600656362
2 - 0.8415757600656362
3 - 0.8415757600656362
4 - 0.8415757600656362
5 - 0.8415757600656362
6 - 0.8415757600656362
7 - 0.8415757600656362
8 - 0.8415757600656362
9 - 0.8415757600656362
10 - 0.8415757600656362
11 - 0.8415757600656362
12 - 0.8415757600656362
13 - 0.8415757600656362
14 - 0.8415757600656362
15 - 0.8415757600656362
16 - 0.8415757600656362
17 - 0.8408855348151687
18 - 0.8405978307880402
19 - 0.840540282198654
20 - 0.840540282198654
21 - 0.840540282198654
22 - 0.840540282198654
23 - 0.840540282198654
24 - 0.8404827310136982
25 - 0.8404827310136982
26 - 0.8407129201816745
27 - 0.8406553767822496
28 - 0.8406553767822496
29 - 0.8406553767822496
30 - 0.8406553767822496
31 - 0.8406553767822496
32 - 0.8407129201816745
33 - 0.8407704609867079
34 - 0.8407204329256038
35 - 0.8406628877723984
36 - 0.8407779754839131
37 - 0.8407779754839131
38 - 0.8407779754839131
39 - 0.8406628877723984
40 - 0.8407204329256038
41 - 0.8407204329256038
42 - 0.8407779754839131
43 - 0.8411731740119234
44 - 0.8415984155241938
45 - 0.8415984155241938
46 - 0.8410557470017146
[44, 45, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 43, 46, 17, 36, 37, 38, 42, 33, 34, 40, 41, 26, 32, 35, 39, 27, 28, 29, 30, 31, 18, 19, 20, 21, 22, 23, 24, 25]
