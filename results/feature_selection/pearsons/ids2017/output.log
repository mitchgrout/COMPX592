#!/usr/bin/env python3

import numpy
import ne
import os
import tee
import warnings
from sklearn.naive_bayes import GaussianNB
warnings.filterwarnings('ignore')

if __name__ ==  '__main__':
    DATASET  = ne.data.ids2017.dataset
    SELECTOR = ne.data.Pearsons
    THRESH   = lambda x: x > 0.5
    
    log_dir = os.path.join('results', 'feature_selection', SELECTOR.name, DATASET.name())
    os.makedirs(log_dir, exist_ok=True)
    with tee.StdoutTee(os.path.join(log_dir, 'output.log'), buff=1):
        ne.util.dump(__file__)

        results = {}
        for n_features in range(1, DATASET.num_features()):
            split = DATASET.data(selector=SELECTOR(n_features), save=False, cache=True)
            model = GaussianNB(var_smoothing=1e-15)
            model.fit(*split.train)
            stats = ne.stats.compute_statistics(THRESH, split.test.ys, model.predict(split.test.xs))
            results[n_features] = stats
            print("{} - {}".format(n_features, stats.mcc))
        print(list(map(lambda t:t[0], sorted(results.items(), key=lambda t:t[1].mcc, reverse=True))))


1 - 0.6100547367015715
2 - 0.5559354244575643
3 - 0.5474778917377054
4 - 0.5429545882079556
5 - 0.5300975459954348
6 - 0.5244195306490641
7 - 0.5375960600781421
8 - 0.5339106704455777
9 - 0.5316276607801755
10 - 0.5356888765600618
11 - 0.5297425808604569
12 - 0.5103601825687163
13 - 0.5034523648218739
14 - 0.5040089757919104
15 - 0.5063989027892509
16 - 0.5077719938636452
17 - 0.505327858038799
18 - 0.5061949129421364
19 - 0.5066480262329893
20 - 0.5036178023786433
21 - 0.5022484206647576
22 - 0.5010108741021915
23 - 0.5022057662744944
24 - 0.5022082172776952
25 - 0.5060017011852607
26 - 0.506711040841985
27 - 0.5058483576529546
28 - 0.5059864231855665
29 - 0.5024422723813219
30 - 0.5042456009840423
31 - 0.5050049251998735
32 - 0.5057177261220804
33 - 0.5120996669853957
34 - 0.5121546873210292
35 - 0.5068044744844854
36 - 0.5251045307978852
37 - 0.5219902803569897
38 - 0.5172271750386792
39 - 0.5202742320598531
40 - 0.5197651991475396
41 - 0.5197852999751834
42 - 0.522724658656501
43 - 0.5222982993228564
44 - 0.5171862926005488
45 - 0.5187119535989767
46 - 0.5179457824240002
47 - 0.5028753068443154
48 - 0.5086182162244173
49 - 0.5099447765952536
50 - 0.5105594129032854
51 - 0.5068067500681886
52 - 0.5031157484706752
53 - 0.5025575780480096
54 - 0.5244569109778988
55 - 0.42398720067015117
56 - 0.4241012832377884
57 - 0.38893232527210947
58 - 0.35642639515774377
59 - 0.33254310346204974
60 - 0.31269935538127785
61 - 0.3112819508047916
62 - 0.2943357627895777
63 - 0.2538792157548115
64 - 0.23294750277029377
65 - 0.23035807702882208
66 - 0.2275817825198908
67 - 0.21475896454862123
68 - 0.1936501957108934
69 - 0.17477598465812522
70 - 0.14446557841251517
71 - 0.14446557841251517
72 - 0.14446557841251517
73 - 0.14446557841251517
74 - 0.14446557841251517
75 - 0.14446557841251517
76 - 0.14446557841251517
77 - 0.14446557841251517
[1, 2, 3, 4, 7, 10, 8, 9, 5, 11, 36, 54, 6, 42, 43, 37, 39, 41, 40, 45, 46, 38, 44, 34, 33, 50, 12, 49, 48, 16, 51, 35, 26, 19, 15, 18, 25, 28, 27, 32, 17, 31, 30, 14, 20, 13, 52, 47, 53, 29, 21, 24, 23, 22, 56, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
