#!/usr/bin/env python3

import numpy
import ne
import os
import tee
import warnings
from sklearn.naive_bayes import GaussianNB
warnings.filterwarnings('ignore')

if __name__ ==  '__main__':
    DATASET  = ne.data.ids2017.dataset
    SELECTOR = ne.data.PCA
    THRESH   = lambda x: x > 0.5
    
    log_dir = os.path.join('results', 'feature_selection', SELECTOR.name, DATASET.name())
    os.makedirs(log_dir, exist_ok=True)
    with tee.StdoutTee(os.path.join(log_dir, 'output.log'), buff=1):
        ne.util.dump(__file__)

        results = {}
        for n_features in range(1, DATASET.num_features()):
            split = DATASET.data(selector=SELECTOR(n_features), save=False, cache=True)
            model = GaussianNB(var_smoothing=1e-15)
            model.fit(*split.train)
            stats = ne.stats.compute_statistics(THRESH, split.test.ys, model.predict(split.test.xs))
            results[n_features] = stats
            print("{} - {}".format(n_features, stats.mcc))
        print(list(map(lambda t:t[0], sorted(results.items(), key=lambda t:t[1].mcc, reverse=True))))


1 - 0.5050974198575346
2 - 0.5175768989156038
3 - 0.5337899362046448
4 - 0.5597197905384687
5 - 0.569429911827749
6 - 0.5653979313614907
7 - 0.16629482758443911
8 - 0.20918494300505286
9 - 0.18832344012931565
10 - 0.1892182693742764
11 - 0.20628852248136728
12 - 0.18030182893034138
13 - 0.18016101191602976
14 - 0.1794324441997749
15 - 0.17991045190539556
16 - 0.14660107275795578
17 - 0.15843831300671127
18 - 0.1619820737817024
19 - 0.16474100265890418
20 - 0.17294260451126287
21 - 0.17539239050183897
22 - 0.18556545042730524
23 - 0.19057809094609343
24 - 0.20220115298768374
25 - 0.20189061972439207
26 - 0.20113016895331617
27 - 0.20443774102978912
28 - 0.20437872807981852
29 - 0.2103271948042362
30 - 0.21188171864573235
31 - 0.21216558394669768
32 - 0.19140166924484225
33 - 0.12076461483609398
