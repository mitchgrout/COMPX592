"""
ne.data
    Functions for processing and splitting data
"""

import numpy
from collections import namedtuple

# Helper tuples for representing feature selectors
class Selector:
    name = None
    def __init__(self, n_features):
        self.n_features = n_features
    def num_features(self):
        return self.n_features

class Pearsons(Selector): name = 'pearsons'
class PCA     (Selector): name = 'pca'

# Helpers tuples for splitting data
Pair  = namedtuple(typename='Pair',  field_names=['xs', 'ys'])
Split = namedtuple(typename='Split', field_names=['train', 'test', 'val'])

# Generic representation of a dataset. Provides simple loading functionality
class Dataset:
    def __init__(self, name, loader, labels):
        self._name   = name
        self._loader = loader
        self._labels = labels
        self._data   = None

    def name(self):
        return self._name

    def labels(self):
        return self._labels

    def num_features(self):
        return len(self._labels) - 1

    def data(self, selector=None, save=False, cache=False, ratio=(0.8, 0.1, 0.1)):
        data = self._load_data(selector, save, cache)
        xs   = data[:, :-1]
        ys   = data[:,  -1]
        return self._make_split(xs, ys, ratio)

    def _load_data(self, selector, save, cache): 
        from os.path               import exists
        from sklearn.preprocessing import StandardScaler

        # Available via cache
        if selector is None and self._data is not None:
            return self._data

        # Available via file
        if exists(self._filename(selector)):
            data = numpy.load(self._filename(selector))          
            if selector is None and cache:
                self._data = data
            return data

        # Unprocessed data is generated by evaluting loader()
        elif selector is None:
            # Use a consistent seed
            numpy.random.seed(2019_2020)
        
            # Would be great if we knew the number of items ahead of time...
            data  = numpy.empty(shape=(0, self.num_features() + 1))
            queue = []
            for arr in self._loader():
                queue.append(arr)
                if len(queue) >  128_000:
                    data = numpy.append(data, numpy.asarray(queue), axis=0)
                    queue.clear()
            if len(queue) > 0:
                data = numpy.append(data, numpy.asarray(queue), axis=0)
                queue.clear()
            # data = numpy.asarray(list(self._loader()))

            # Shuffle
            data = data[numpy.random.permutation(data.shape[0])]
            
            # Feature normalization
            data[:, :-1] = StandardScaler().fit_transform(data[:, :-1])

            if save:
                numpy.save(self._filename(selector), data)
            if cache:
                self._data = data
            return data           

        # Processed data required the unprocessed data first
        else:
            data = self._load_data(None, save, cache)

            # Choose our feature selector
            if selector.name == 'pearsons':
                from sklearn.feature_selection import SelectKBest, f_classif
                s = SelectKBest(score_func=f_classif, k=selector.n_features)
            elif selector.name == 'pca':
                from sklearn.decomposition import PCA
                s = PCA(n_components=selector.n_features)
            else:
                raise Exception('Unknown feature selector {}'.format(type(selector)))

            # Transform [slicing makes recombining easier]
            new_xs = s.fit_transform(data[:, :-1], data[:, -1])
            new_xs = StandardScaler().fit_transform(new_xs) # NOTE: Check we need this double-norm
            data = numpy.append(new_xs, data[:, -1:], axis=1)

            if save:
                numpy.save(self._filename(selector), data)
            return data
        pass
        
    def _filename(self, selector):
        from os      import makedirs
        from os.path import join

        if selector is None:
            # Unprocessed data is in ne/data/name/data.npy
            d = ['data.npy']
        else:
            # Processed data is in ne/data/name/selector/data_n.npy
            d = [ selector.name, 'data_{}.npy'.format(selector.n_features) ]
        makedirs(join('ne', 'data', self._name, *d[:-1]), exist_ok=True)
        return   join('ne', 'data', self._name, *d)

    def _make_split(self, xs, ys, ratio):
        assert 0.0 <= sum(ratio)
        assert sum(ratio) <= 1.0
        a = round(xs.shape[0] * ratio[0])
        b = round(xs.shape[0] * ratio[1]) + a
        c = round(xs.shape[0] * ratio[2]) + b
        return Split(train=Pair(xs=xs[ :a], ys=ys[ :a]),
                     test =Pair(xs=xs[a:b], ys=ys[a:b]),
                     val  =Pair(xs=xs[b:c], ys=ys[b:c]))

def _load_random():
    from sklearn.datasets import make_classification
    xs, ys = make_classification(n_samples=100, n_features=20, n_informative=5)
    for i in range(100):
        yield xs[i] + [ ys[i] ]
random = Dataset('random', _load_random, [''] * 20)

def _load_mnist():
    from sklearn.datasets import load_digits
    xs, ys = load_digits(return_X_y=True) 
    for i in range(xs.shape[0]):
        yield xs[i] + [ ys[i] ]
mnist = Dataset('mnist', _load_mnist, [''] * 64)
